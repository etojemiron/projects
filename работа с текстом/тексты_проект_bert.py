# -*- coding: utf-8 -*-
"""тексты проект BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tU-JyV8_fIhAIPE44hHEN8J-xFMsirqh

## ПРОЕКТ С BERT

##1. Подготовка
"""

!pip install transformers

import pandas as pd
import nltk
import numpy as np
import transformers
from tqdm import notebook
from nltk.corpus import stopwords as nltk_stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import f1_score, make_scorer, mean_squared_error, accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import re
from sklearn.model_selection import train_test_split
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

from IPython.display import clear_output

from keras.preprocessing.sequence import pad_sequences

import torch

from transformers import BertModel, BertConfig, BertTokenizer, BertForSequenceClassification, BertForTokenClassification, AdamW

import torch
from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler

df = pd.read_csv('drive/MyDrive/toxic_comments.csv')

df

"""Данные на базе)"""

df['text'] = df['text'].values.astype('U')

df['text'] = df['text'].str.lower()

"""Приводим все слова к нижнему регистру"""

clear = []
for i in df.text:
  cleared = re.sub(r'[^a-zA-Z0-9]'," ", i)
  clear.append(' '.join(cleared.split()))

df['clear_text']=clear

df

"""делаем столбец с чистым текстом, без знаков, только буквы и цифры"""

corpus = df.sample(20000,random_state=42).reset_index(drop=True)

"""возьмем сэмп данных, чтобы было не так много данных, проверим соотношение классов в сэмпле"""

print(df.toxic.value_counts()/df.shape[0]*100)
print(corpus.toxic.value_counts()/corpus.shape[0]*100)

"""будем использовать WordNetLemmatizer, проводим лемматизацию и добавляем в новый столбец лемматизированный текст"""

L = WordNetLemmatizer()

def lemmatizer(corpus):
  corpus_new = []
  for i in corpus:
    word_list = nltk.word_tokenize(i)
    corpus_new.append(' '.join([L.lemmatize(w) for w in word_list]))
  return corpus_new

nltk.download('stopwords')
nltk.download('wordnet')

def get_wordnet_pos(word):
  tag = nltk.pos_tag([word])[0][1][0].upper()
  tag_dict = {'J': wordnet.ADJ,
              'N': wordnet.NOUN,
              'V': wordnet.VERB,
              'R': wordnet.ADV}
  return tag_dict.get(tag, wordnet.NOUN)

def get_word_text(corpus):
  corpus_new = []
  for i in corpus:
    corpus_new.append(' '.join([L.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(i) if not w in stopwords.words('english')]))
  return corpus_new

!pip install nltk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

corpus['lemma_text'] = get_word_text(corpus['clear_text'])

corpus

"""Разделяем на тренировочную и тестовую выборки, создаем фичи и таргет"""

train = []
for i in corpus['lemma_text']:
  train.append(i.split())
corpus['split'] = train

train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42,stratify = corpus['toxic'])

train_corpus

count_tf_idf = TfidfVectorizer(stop_words = 'english')
tf_idf_train = count_tf_idf.fit_transform(train_corpus['lemma_text'])
tf_idf_test = count_tf_idf.transform(test_corpus['lemma_text'])

f_train = tf_idf_train
f_test = tf_idf_test
t_train = train_corpus['toxic'].values
t_test = test_corpus['toxic'].values

"""##2. Обучение и проверка моделей"""

lr = LogisticRegressionCV()
lr.fit(f_train,t_train)
f1_lr = f1_score(lr.predict(f_test),t_test)
print(f1_lr)

dtc = DecisionTreeClassifier(random_state=42,max_depth=67)
dtc.fit(f_train,t_train)
f1_dtc = f1_score(dtc.predict(f_test),t_test)
print(f1_dtc)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# forest = RandomForestClassifier(random_state=42)
# forest.fit(f_train,t_train)
# f1_forest = f1_score(forest.predict(f_test),t_test)
# print(f1_forest)

"""## BERT"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

if device == torch.device('cpu'):
    print('CPU')
else:
    gpus = torch.cuda.device_count()
    print('{} GPUs'.format(torch.cuda.get_device_name(0)))

sentences = corpus['clear_text'].values
sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in sentences]
labels = corpus['toxic'].values

assert len(sentences) == len(labels)

train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, test_size=0.2)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenized_text = [tokenizer.tokenize(sent) for sent in train_sentences]

print(tokenized_text[1])

max_len_sen = 150
batch_size = 35
input_ids = [tokenizer.convert_tokens_to_ids(x[:150]) for x in tokenized_text]
input_ids = pad_sequences(
    input_ids,
    maxlen = max_len_sen,
    dtype = 'long',
    truncating = 'post',
    padding = 'post'
)
attention_masks = []
attention_masks = [[float(i>0) for i in seq] for seq in input_ids]

train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(
    input_ids, 
    train_gt,
    random_state=42,
    test_size=0.1
)
train_masks, validation_masks, _, _ = train_test_split(
    attention_masks, 
    input_ids,
    random_state=42,
    test_size=0.1
)

train_inputs = torch.tensor(train_inputs)
train_labels = torch.tensor(train_labels)
train_masks = torch.tensor(train_masks)
validation_inputs = torch.tensor(validation_inputs)
validation_labels = torch.tensor(validation_labels)
validation_masks = torch.tensor(validation_masks)

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data,sampler=RandomSampler(train_data),batch_size=60)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_dataloader = DataLoader(validation_data,sampler=RandomSampler(validation_data),batch_size=60)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)
model.to(device)

param_optimizer = list(model.named_parameters())
no_decay = ['bias','gamma','beta']
optimizer_grouped_parametrs = [
                 {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate':0.01},
                 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate':0.0},             
]
optimizer = AdamW(optimizer_grouped_parametrs,lr=2e-5)

train_loss_set = []
train_loss = 0
model.train()
for step, batch in enumerate(train_dataloader):
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch
  b_input_ids = torch.tensor(b_input_ids).to(torch.int64)
  optimizer.zero_grad()
  loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
  train_loss_set.append(loss[0].item())
  loss[0].backward()
  optimizer.step()
  train_loss += loss[0].item()
  clear_output(True)
  plt.plot(train_loss_set)
  plt.title('TRL')
  plt.xlabel('Batch')
  plt.ylabel('Loss')
  plt.show()
print('Loss на обучающей: {0:.5f}'.format( train_loss / len(train_dataloader)))

model.eval()
valid_preds, valid_labels = [], []
for batch in validation_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch
  b_input_ids = torch.tensor(b_input_ids).to(torch.int64)
  with torch.no_grad():
    logits = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask)
  logits = logits[0].detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  batch_preds = np.argmax(logits, axis = 1)
  batch_labels = np.array(label_ids)
  valid_preds.extend(batch_preds)
  valid_labels.extend(batch_labels)
print('Правильно на валидации: {0:.2f}%'.format(accuracy_score(valid_labels, valid_preds) * 100))
print('f1_score: {0:.2f}%'.format(f1_score(valid_labels, valid_preds)))

tokenized_text = [tokenizer.tokenize(sent) for sent in test_sentences]
input_ids = [tokenizer.convert_tokens_to_ids(x[:max_len_sen]) for x in tokenized_text]
input_ids = pad_sequences(
    input_ids,
    maxlen = max_len_sen,
    dtype = 'long',
    truncating = 'post',
    padding = 'post'
)
attention_masks = [[float(i>0) for i in seq] for seq in input_ids]
prediction_inputs = torch.tensor(input_ids)
prediction_labels = torch.tensor(test_gt)
prediction_masks = torch.tensor(attention_masks)
prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)
prediction_dataloader = DataLoader(prediction_data,sampler=SequentialSampler(prediction_data),batch_size=32)

model.eval()
test_preds, test_labels = [], []
for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  b_input_ids, b_input_mask, b_labels = batch
  b_input_ids = torch.tensor(b_input_ids).to(torch.int64)
  with torch.no_grad():
    logits = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask)
  logits = logits[0].detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()
  batch_preds = np.argmax(logits, axis = 1)
  batch_labels = np.array(label_ids)
  test_preds.extend(batch_preds)
  test_labels.extend(batch_labels)
print(f1_score(test_labels, test_preds))

"""##4. Выводы

Начальные данные были обработаны, лемматизированы, разделены на выборки.

Обучены разные модели, ф1 скор лучше всего показала модель Bert'а. 

Результат лучшего ф1 скора 0.835
"""