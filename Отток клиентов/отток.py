# -*- coding: utf-8 -*-
"""отток.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B1BykONo9dBsEkRSTIj46MZehhdmOWwo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from math import factorial
from scipy import stats as st
from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
pd.options.mode.chained_assignment = None
from sklearn.preprocessing import StandardScaler
import sys
import warnings
if not sys.warnoptions:
       warnings.simplefilter("ignore")

dfc = pd.read_csv('/datasets/c.csv')
dfi = pd.read_csv('/datasets/i.csv')
dfp = pd.read_csv('/datasets/p.csv')
dfph = pd.read_csv('/datasets/ph.csv')

"""Выгружаем данные."""

dfc

"""Сделаем из столбца "EndDate" целевой признак выхода клиента где 1 ушел, 0 остался. Сделаем новый признак "Времени жизни клиента", вычитая из срока окончания дату начала, заменив значения "Нет" на актуальную дату."""

dfc.loc[dfc['BeginDate']=='2020-02-01']

"""Есть договоры с датой заключения на момент сбора данных, это выльется потом в NaN в TotalCharges, но так как это активные клиенты, удалять не будем, просто запишем в вместо нанов 0(позже)."""

dfc.info()

"""Сколько всего заплатили клиенты нужно изменить формат, также с датами. у нового признака оставим численный формат в кол-ве дней. После этого удалим колонки с датами."""

dfc['TotalCharges'] = pd.to_numeric(dfc['TotalCharges'], errors='coerce')

dfc['BeginDate'] = pd.to_datetime(dfc['BeginDate'], format='%Y-%m-%d')

dfc.loc[dfc['EndDate'] == 'No', 'Exited'] = 0
dfc.loc[dfc['EndDate'] != 'No', 'Exited'] = 1
dfc['Exited'] = dfc['Exited'].astype('int')

dfc.loc[dfc['EndDate'] == 'No', 'EndDate'] = '2020-02-01'
dfc['EndDate'] = pd.to_datetime(dfc['EndDate'], format='%Y-%m-%d')
dfc['LTV'] = dfc['EndDate'] - dfc['BeginDate']
dfc['LTV'] = dfc['LTV'].dt.days

dfc.drop(['EndDate'], axis='columns', inplace=True)
dfc.drop(['BeginDate'], axis='columns', inplace=True)

dfc.info()

dfc

dfc.duplicated().sum()

dfc['Type'].value_counts()

dfc['PaperlessBilling'].value_counts()

dfc['PaymentMethod'].value_counts()

dfp.info()

dfp['gender'].value_counts()

dfp['SeniorCitizen'].value_counts()

dfp['Partner'].value_counts()

dfp['Dependents'].value_counts()

df1 = dfc.merge(dfp,on=['customerID'])

df1

dfi

dfi.info()

dfi['InternetService'].value_counts()

dfi['OnlineSecurity'].value_counts()

dfi['OnlineBackup'].value_counts()

dfi['DeviceProtection'].value_counts()

dfi['TechSupport'].value_counts()

dfi['StreamingTV'].value_counts()

dfi['StreamingMovies'].value_counts()

df2 = df1.join(dfi.set_index('customerID'), on='customerID')

df2

df2.isna().sum()

"""Те самые заключенные договора на 2020-02-01."""

df2['TotalCharges'] = df2['TotalCharges'].fillna(0)

df2.loc[df2['InternetService'].isna()]

df2['InternetService'] = df2['InternetService'].fillna('None')
df2['OnlineSecurity'] = df2['OnlineSecurity'].fillna('None')
df2['OnlineBackup'] = df2['OnlineBackup'].fillna('None')
df2['DeviceProtection'] = df2['DeviceProtection'].fillna('None')
df2['TechSupport'] = df2['TechSupport'].fillna('None')
df2['StreamingTV'] = df2['StreamingTV'].fillna('None')
df2['StreamingMovies'] = df2['StreamingMovies'].fillna('None')

df2.isna().sum()

"""Заменяем пропущенные значения в сервисах и типах сервиса на 'None', чтобы выделить этих пользователей в группу, которая не пользуется интернетом, если заменить на 'No', будет казаться, что они подключены 3м способом, но не пользуются сервисами."""

dfph

dff = df2.join(dfph.set_index('customerID'), on='customerID')

dff

dff['MultipleLines'] = dff['MultipleLines'].fillna('None')

dff.isna().sum()

dff.info()

"""Также в телефонии заменяем пустые строки на отдельные значения, чтобы выделить эту группу."""

dff = dff.drop(['customerID'],axis=1)

"""Проведем кодирование."""

df_ohe = pd.get_dummies(dff, drop_first=True)

df_ohe.info()

"""Разделим данные на тест и обучение."""

df_train, df_test = train_test_split(df_ohe, test_size=0.2, random_state=42)
print(df_train.shape)
print(df_test.shape)

features_train = df_train.drop(['Exited'],axis=1).reset_index(drop=True)
target_train = df_train['Exited'].reset_index(drop=True)
features_test = df_test.drop(['Exited'], axis=1).reset_index(drop=True)
target_test = df_test['Exited'].reset_index(drop=True)

"""Обучим модель случайного леса с разными параметрами и выберем лучшую."""

best_model_rfc = None
best_result_rfc = 0
best_estimators = 0
best_depth = 0
for est in range(10,200,20):
  for depth in range(1,50,5):
    model = RandomForestClassifier(random_state=42,n_estimators=est,max_depth=depth,class_weight='balanced',criterion='gini',min_weight_fraction_leaf=0.01)
    model.fit(features_train,target_train) 
    result = model.score(features_test,target_test) 
    if result > best_result_rfc:
        best_model_rfc = model 
        best_result_rfc = result
        best_estimators = est 
        best_depth = depth          
print(best_result_rfc, best_estimators, best_depth)

accuracy_score(target_test,best_model_rfc.predict(features_test))

f1_score(target_test,best_model_rfc.predict(features_test))

model = best_model_rfc
model.fit(features_train, target_train)
probabilities_test = model.predict_proba(features_test)
probabilities_one_test = probabilities_test[:, 1]

for threshold in np.arange(0, 1, 0.1):
    predicted_test = probabilities_one_test > threshold
    precision = precision_score(target_test,predicted_test)
    recall = recall_score(target_test,predicted_test)

    print("Порог = {:.2f} | Точность = {:.3f}, Полнота = {:.3f}".format(
        threshold, precision, recall))

auc_roc = roc_auc_score(target_test,probabilities_one_test)
print(auc_roc)

"""Метрика AUC-ROC нормальная, попробуем со стандартизацией численных признаков."""

numeric = ['MonthlyCharges','TotalCharges','LTV']

scaler = StandardScaler()
scaler.fit(features_train[numeric]) 
features_train[numeric] = scaler.transform(features_train[numeric])
features_test[numeric] = scaler.transform(features_test[numeric])

model = best_model_rfc
model.fit(features_train, target_train)
probabilities_test = model.predict_proba(features_test)
probabilities_one_test = probabilities_test[:, 1]
auc_roc = roc_auc_score(target_test,probabilities_one_test)
print(auc_roc,accuracy_score(target_test,model.predict(features_test)))

"""Метрика AUC-ROC стала совсем чуть лучше."""

def chart_features(model):
    feature_imp = pd.Series(model.feature_importances_, index=features_test.columns).sort_values(ascending=False)
    plt.figure(figsize=(15,15))
    ax = sns.barplot(x=feature_imp, y=feature_imp.index)
    _ = ax.set(xlabel='Оценка важности признаков', ylabel='Признаки')
    _ = ax.set_title('Визуализация важности признаков')

chart_features(best_model_rfc)

"""Основным признаком был наш новый созданный Время жизни клиента, на 3м месте сколько всего денег отдал клиент, размер оплаты в месяц на 5м месте. Посмотрим на бустинг."""

from lightgbm import LGBMModel,LGBMClassifier

model = LGBMClassifier()
model.fit(features_train, target_train)
probabilities_test = model.predict_proba(features_test)
probabilities_one_test = probabilities_test[:, 1]
auc_roc = roc_auc_score(target_test,probabilities_one_test)
print(auc_roc,accuracy_score(target_test,model.predict(features_test)))

"""Хорошая AUC-ROC метрика и accuracy выше чем у леса, что ожидаемо."""

chart_features(model)

"""Видим что бустинг брал за основные признаки Время жизни клиента, и его выплаты за месяц и общую выплату, что на мой вгляд более логично, ведь от цен зависит уйдет ли он к другим или нет."""

df_ohe

dfd = df_ohe.drop(['StreamingMovies_None','StreamingTV_None','TechSupport_None','DeviceProtection_None','OnlineBackup_None','OnlineSecurity_None'],axis=1).reset_index(drop=True)

dfd

plt.figure(figsize=(20,20))
sns.heatmap(dfd.corr(), annot=True, fmt='.1g', cmap= 'coolwarm', square=True)

dfd = dfd.drop(['InternetService_None','InternetService_Fiber optic'],axis=1).reset_index(drop=True)

df_train, df_test = train_test_split(dfd, test_size=0.2, random_state=42)
print(df_train.shape)
print(df_test.shape)

features_train = df_train.drop(['Exited'],axis=1).reset_index(drop=True)
target_train = df_train['Exited'].reset_index(drop=True)
features_test = df_test.drop(['Exited'], axis=1).reset_index(drop=True)
target_test = df_test['Exited'].reset_index(drop=True)

model = LGBMClassifier()
model.fit(features_train, target_train)
probabilities_test = model.predict_proba(features_test)
probabilities_one_test = probabilities_test[:, 1]
auc_roc = roc_auc_score(target_test,probabilities_one_test)
print('AUC_ROC:',auc_roc,'Accuracy:',accuracy_score(target_test,model.predict(features_test)),'Полнота:',precision_score(target_test,model.predict(features_test)))

"""После этого метрики улучшились, но осталась корреляция Времени и Месячной оплаты с Total'ом, так как он на 3м месте значимости и по логике, это все лишь деньги в месяц умноженные как раз на время жизни его тоже удалил, заодно убрал не особо важные признаки с корреляцией 0.6"""

dfd3 = dfd.drop(['TotalCharges','StreamingMovies_Yes','StreamingTV_Yes'],axis=1).reset_index(drop=True)

df_train, df_test = train_test_split(dfd3, test_size=0.2, random_state=42)
print(df_train.shape)
print(df_test.shape)

features_train = df_train.drop(['Exited'],axis=1).reset_index(drop=True)
target_train = df_train['Exited'].reset_index(drop=True)
features_test = df_test.drop(['Exited'], axis=1).reset_index(drop=True)
target_test = df_test['Exited'].reset_index(drop=True)

modelLG = LGBMClassifier()
modelLG.fit(features_train, target_train)
probabilities_test = modelLG.predict_proba(features_test)
probabilities_one_test = probabilities_test[:, 1]
auc_roc = roc_auc_score(target_test,probabilities_one_test)
print('AUC_ROC:',auc_roc,'Accuracy:',accuracy_score(target_test,modelLG.predict(features_test)),'Полнота:',precision_score(target_test,modelLG.predict(features_test)))

plt.figure(figsize=(20,20))
sns.heatmap(dfd3.corr(), annot=True, fmt='.1g', cmap= 'coolwarm', square=True)

"""В итоге метрики вышли еще лучше, а критичных корреляций не осталось."""